# OpenAI API key configuration
openai_api_key: "sk-proj-111"

# Ollama local models configuration
models:
  llama_3_1_8b_instruct_q8_0: "ollama/llama3.1:8b-instruct-q8_0"
  llama_3_1_8b_instruct_q5_K_S: "ollama/llama3.1:8b-instruct-q5_K_S"
  llama3_1_8b_instruct_q3_K_S: "ollama/llama3.1:8b-instruct-q3_K_S"
  llama3_2_3b_instruct_q5_K_S: "ollama/llama3.2:3b-instruct-q5_K_S"
  llama3_2_1b_instruct_q5_K_S: "ollama/llama3.2:1b-instruct-q5_K_S"
  gemma2_27b_instruct_q2_K: "ollama/gemma2:27b-instruct-q2_K" # resource-intensive
  gemma2_9b_instruct_q5_K_S: "ollama/gemma2:9b-instruct-q5_K_S"
  gemma2_9b_instruct_q8_0: "ollama/gemma2:9b-instruct-q8_0"
  gemma2_2b_text_q5_K_S: "ollama/gemma2:2b-instruct-q5_K_S"
  gemma2_2b_instruct_q4_K_S: "ollama/gemma2:2b-instruct-q4_K_S"
  mistral12b_model: "ollama/mistral-nemo:12b-instruct-2407-q3_K_M" # resource-intensive
  mistral_nemo_12b_instruct_2407_q8_0: "ollama/mistral-nemo:12b-instruct-2407-q8_0" # resource-intensive
  mistral_nemo_12b_instruct_2407_q5_K_S: "ollama/mistral-nemo:12b-instruct-2407-q5_K_S"
embeddings:
  provider: "ollama"
  config:
    nomic_text_model: "nomic-embed-text:latest"
    mxbai_text_model: "ollama/mxbai-embed-large:latest"

# LmStudio local models and server configuration
server:
  url: "http://127.0.0.1:1234/v1/completions"

request:
  lmstudio_models:
    gemma-2-9b-it-GGUF: "gemma-2-9b-instruct"
    gemma-2-9b-it-Q5_K_M.gguf: "gemma-2-9b-instruct"
    Mistral-Nemo-Gutenberg-Doppel-12B-v2.Q4_0.gguf: "mistral-nemo-gutenberg-doppel-12b-v2"
    Llama-3.2-3B-Instruct-Q8_0-GGUF: "lama-3.2-3b-instruct-q8_0.gguf"
    Llama-3-Groq-8B-Tool-Use-GGUF: "llama-3-groq-8b-tool-use"
    Meta-Llama-3.1-8B-Instruct-GGUF: "meta-llama-3.1-8b-instruct"
    Meta-Llama-3.1-70B-Instruct-Q2_K.gguf: "meta-llama-3.1-70b-instruct"
    mathstral-7B-v0.1-GGUF: "mathstral-7b-v0.1" 
    Phi-3.5-mini-instruct-Q6_K_L.gguf: "phi-3.5-mini-instruct" #--> Assumiu papel de gerente
    Qwen2.5-14B-Instruct-Q4_K_M.gguf: "qwen2.5-14b-instruct"
